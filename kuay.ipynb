{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "from torch import nn\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0123456789.-+ABCDEFGHIJKLMNOPQRSTUVWXYZ/\\abcdefghijklmnopqrstuvwxyz,!@#$%^&*()?:;'\"~`\n",
      "{' ': 0, '0': 1, '1': 2, '2': 3, '3': 4, '4': 5, '5': 6, '6': 7, '7': 8, '8': 9, '9': 10, '.': 11, '-': 12, '+': 13, 'A': 14, 'B': 15, 'C': 16, 'D': 17, 'E': 18, 'F': 19, 'G': 20, 'H': 21, 'I': 22, 'J': 23, 'K': 24, 'L': 25, 'M': 26, 'N': 27, 'O': 28, 'P': 29, 'Q': 30, 'R': 31, 'S': 32, 'T': 33, 'U': 34, 'V': 35, 'W': 36, 'X': 37, 'Y': 38, 'Z': 39, '/': 40, '\\\\': 41, 'a': 42, 'b': 43, 'c': 44, 'd': 45, 'e': 46, 'f': 47, 'g': 48, 'h': 49, 'i': 50, 'j': 51, 'k': 52, 'l': 53, 'm': 54, 'n': 55, 'o': 56, 'p': 57, 'q': 58, 'r': 59, 's': 60, 't': 61, 'u': 62, 'v': 63, 'w': 64, 'x': 65, 'y': 66, 'z': 67, ',': 68, '!': 69, '@': 70, '#': 71, '$': 72, '%': 73, '^': 74, '&': 75, '*': 76, '(': 77, ')': 78, '?': 79, ':': 80, ';': 81, \"'\": 82, '\"': 83, '~': 84, '`': 85}\n"
     ]
    }
   ],
   "source": [
    "with open('data/raw/ittk5/alphabet.txt') as f:\n",
    "    alphabet = f.readline()\n",
    "print(alphabet)    \n",
    "\n",
    "# Map the characters in the alphabet to the index\n",
    "alphabet_map = {}\n",
    "for i, char in enumerate(alphabet):\n",
    "    # The index of blank in CTCLoss should be zero.\n",
    "    # The first one in the alphabet has been left blank,\n",
    "    # and there is no need for special operation here\n",
    "    alphabet_map[char] = i\n",
    "print(alphabet_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "\n",
    "def rename_and_save_images(mode):\n",
    "    if mode == 'train':\n",
    "        img_dir = 'data/raw/ittk5/train/img'\n",
    "        label_dir = 'data/raw/ittk5/train/label'\n",
    "        dest_dir = 'data/processed/ittk5/train/'\n",
    "    elif mode == 'test':\n",
    "        img_dir = 'data/raw/ittk5/test/img'\n",
    "        label_dir = 'data/raw/ittk5/test/label'\n",
    "        dest_dir = 'data/processed/ittk5/test/'\n",
    "\n",
    "    os.makedirs(dest_dir, exist_ok=True)\n",
    "\n",
    "    for img_name in os.listdir(img_dir):\n",
    "        if img_name.endswith('.jpg'):\n",
    "            base_name = os.path.splitext(img_name)[0]\n",
    "            label_file = os.path.join(label_dir, f\"{base_name}.txt\")\n",
    "            if os.path.exists(label_file):\n",
    "                try:\n",
    "                    with open(label_file, 'r') as f:\n",
    "                        label = f.read().strip()\n",
    "                    # Sanitize the label to remove slashes and other invalid characters\n",
    "                    sanitized_label = label.replace('/', '_')\n",
    "                    new_img_name = f\"{base_name}_{sanitized_label}.jpg\"\n",
    "                    shutil.copy(\n",
    "                        os.path.join(img_dir, img_name),\n",
    "                        os.path.join(dest_dir, new_img_name)\n",
    "                    )\n",
    "                except Exception as e:\n",
    "                    print(f\"Skipping {img_name}: {e}\")\n",
    "                    continue\n",
    "\n",
    "rename_and_save_images(mode='train')\n",
    "rename_and_save_images(mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    \"\"\"Create dataset inherited from torch.utils.data.Dataset\n",
    "    \n",
    "    Attributes:\n",
    "        data_dir: train dir or test dir.\n",
    "        alphabet_map: The map from char to index.\n",
    "        img_names: File names of all image under the data_dir.\n",
    "        lables: Labels of all image under the data_dir.\n",
    "        trans: Converts a PIL Image or numpy.ndarray (H x W x C) in the range [0, 255]\n",
    "        to a torch.FloatTensor of shape (C x H x W) in the range [0.0, 1.0]\n",
    "    \n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir):\n",
    "        \"\"\"Inits dataset\"\"\"\n",
    "        self.data_dir = data_dir\n",
    "        self.alphabet_map = alphabet_map\n",
    "        self.img_names = os.listdir(self.data_dir)\n",
    "        self.labels = [i.split('_')[1].split('.')[0] for i in self.img_names]\n",
    "        #print(f'label: {self.labels}')\n",
    "        self.trans = transforms.Compose([\n",
    "            transforms.Grayscale(num_output_channels=1),\n",
    "            transforms.Resize((32, 128)),\n",
    "            transforms.RandomRotation(5),\n",
    "            transforms.RandomAffine(0, shear=10, scale=(0.8, 1.2)),\n",
    "            transforms.ToTensor()\n",
    "        ])\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        \"\"\"Get single image by idx\n",
    "        \n",
    "        Args:\n",
    "            idx: index\n",
    "            \n",
    "        Returns:\n",
    "            img: torch.FloatTensor\n",
    "            label: Actual lable of the image, like \"ZOW-PRF-LFB\".\n",
    "        \"\"\"\n",
    "        img_path = os.path.join(self.data_dir, self.img_names[idx])\n",
    "        img = Image.open(img_path)\n",
    "        img = self.trans(img)\n",
    "        label = self.labels[idx]\n",
    "        #print(f'label: {label}')\n",
    "        return img, label\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    \n",
    "class BiLSTM(nn.Module):\n",
    "    \"\"\" Bidirectional LSTM and embedding layer.\n",
    "    \n",
    "    Attributes:\n",
    "        rnn: Bidirectional LSTM\n",
    "        linear: Embedding layer\n",
    "    \"\"\"\n",
    "    def __init__(self, num_input, num_hiddens, num_output):\n",
    "        super().__init__()\n",
    "        self.rnn = nn.LSTM(num_input, num_hiddens, bidirectional=True)\n",
    "        # the size of input of embedding layer should mutiply by 2, because of the bidirectional.\n",
    "        self.linear = nn.Linear(num_hiddens * 2, num_output)  \n",
    "    \n",
    "    def forward(self, X):\n",
    "        rnn_out, _ = self.rnn(X)\n",
    "        T, b, h = rnn_out.size()  # T: time step, b: batch size, h: hidden size\n",
    "        t_rec = rnn_out.view(T * b, h)\n",
    "        output = self.linear(t_rec)\n",
    "        output = output.view(T, b, -1)\n",
    "        return output\n",
    "\n",
    "\n",
    "class CRNN(nn.Module):\n",
    "    def __init__(self, num_class):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),  # Height: 32 -> 16\n",
    "            nn.Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2, padding=0),  # Height: 16 -> 8\n",
    "            nn.Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1)),  # Height: 8 -> 4\n",
    "            nn.Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1)),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool2d(kernel_size=(2, 2), stride=(2, 1), padding=(0, 1)),  # Height: 4 -> 2\n",
    "            nn.Conv2d(512, 512, kernel_size=(2, 2), stride=(1, 1), padding=(0, 0)),  # Height: 2 -> 1\n",
    "            nn.BatchNorm2d(512),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.Sequential(\n",
    "            BiLSTM(512, 256, 256),\n",
    "            BiLSTM(256, 256, num_class)\n",
    "        )\n",
    "        \n",
    "    def forward(self, X):\n",
    "        cnn_out = self.cnn(X)  # cnn_out shape: (batch_size x channel x height x width)\n",
    "        assert cnn_out.shape[2] == 1, \"the height of conv must be 1\"\n",
    "        cnn_out = cnn_out.squeeze(2)  # squeeze the dim 2 (height) of cnn_out\n",
    "        cnn_out = cnn_out.permute(2, 0, 1)  # move the width to the first dim, as the time step of rnn input\n",
    "        output = self.rnn(cnn_out)  # output shape: (time step x batch_size x num_class)\n",
    "        output = F.log_softmax(output, dim=2)  # do softmax at the dim of num_class\n",
    "        return output\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input shape: torch.Size([64, 1, 32, 128])\n",
      "output shape from CRNNnet: torch.Size([33, 64, 86])\n"
     ]
    }
   ],
   "source": [
    "train_set = MyDataset(data_dir='data/processed/ittk5/train')\n",
    "batch_size = 64\n",
    "trainloader = DataLoader(dataset=train_set, batch_size=batch_size, shuffle=True, drop_last=True)\n",
    "\n",
    "# Check if the input and output shapes meet expectations\n",
    "for X, y in trainloader:\n",
    "    break\n",
    "print('input shape:', X.shape)\n",
    "crnn = CRNN(num_class=len(alphabet))\n",
    "#print(crnn)\n",
    "preds = crnn(X)\n",
    "print('output shape from CRNNnet:', preds.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ctcloss_parameters(text_batch):\n",
    "    \"\"\"Convert the real text batch into three parameters required by ctcloss,\n",
    "    encoded text/predict length/real length\n",
    "    \n",
    "    Args:\n",
    "        text_batch: real text batch, like('E-Z-4', 'EMD-6-04')\n",
    "        \n",
    "    Returns:\n",
    "        encoded_text: encode text by alphabet_map \n",
    "        preds_length: (time step x batch_size) => (51 * batch_size)\n",
    "        actual_length: length of text to index，max(len(text)) * batch_size\n",
    "    \"\"\"\n",
    "    actual_length = []\n",
    "    result = []\n",
    "    for item in text_batch:            \n",
    "        actual_length.append(len(item))\n",
    "        r = []\n",
    "        for char in item:\n",
    "            index = alphabet_map[char]\n",
    "            r.append(index)\n",
    "        result.append(r)\n",
    "\n",
    "    max_len = 0\n",
    "    for r in result:\n",
    "        if len(r) > max_len:\n",
    "            max_len = len(r)\n",
    "\n",
    "    result_temp = []\n",
    "    for r in result:\n",
    "        for i in range(max_len - len(r)):\n",
    "            r.append(0)\n",
    "        result_temp.append(r)\n",
    "\n",
    "    encoded_text = result_temp\n",
    "    encoded_text = torch.LongTensor(encoded_text)\n",
    "    preds_length = torch.LongTensor([preds.size(0)] * batch_size)\n",
    "    actual_length = torch.LongTensor(actual_length)\n",
    "    return encoded_text, preds_length, actual_length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1          loss: 0.069445\n",
      "epoch 2          loss: 0.067837\n",
      "epoch 3          loss: 0.068413\n",
      "epoch 4          loss: 0.067885\n",
      "epoch 5          loss: 0.066891\n",
      "epoch 6          loss: 0.065355\n",
      "epoch 7          loss: 0.065597\n",
      "epoch 8          loss: 0.065623\n",
      "epoch 9          loss: 0.065407\n",
      "epoch 10         loss: 0.063945\n",
      "epoch 11         loss: 0.063161\n",
      "epoch 12         loss: 0.062921\n",
      "epoch 13         loss: 0.062178\n",
      "epoch 14         loss: 0.062293\n",
      "epoch 15         loss: 0.057206\n",
      "epoch 16         loss: 0.058468\n",
      "epoch 17         loss: 0.056464\n",
      "epoch 18         loss: 0.059342\n",
      "epoch 19         loss: 0.057968\n",
      "epoch 20         loss: 0.057134\n",
      "epoch 21         loss: 0.056437\n",
      "epoch 22         loss: 0.056523\n",
      "epoch 23         loss: 0.049896\n",
      "epoch 24         loss: 0.051441\n",
      "epoch 25         loss: 0.048132\n",
      "epoch 26         loss: 0.047355\n",
      "epoch 27         loss: 0.046204\n",
      "epoch 28         loss: 0.044522\n",
      "epoch 29         loss: 0.037848\n",
      "epoch 30         loss: 0.038982\n",
      "epoch 31         loss: 0.034903\n",
      "epoch 32         loss: 0.040317\n",
      "epoch 33         loss: 0.029217\n",
      "epoch 34         loss: 0.032928\n",
      "epoch 35         loss: 0.024082\n",
      "epoch 36         loss: 0.028669\n",
      "epoch 37         loss: 0.028225\n",
      "epoch 38         loss: 0.026112\n",
      "epoch 39         loss: 0.016382\n",
      "epoch 40         loss: 0.022332\n",
      "epoch 41         loss: 0.014788\n",
      "epoch 42         loss: 0.014126\n",
      "epoch 43         loss: 0.014182\n",
      "epoch 44         loss: 0.013501\n",
      "epoch 45         loss: 0.015236\n",
      "epoch 46         loss: 0.011482\n",
      "epoch 47         loss: 0.009288\n",
      "epoch 48         loss: 0.009305\n",
      "epoch 49         loss: 0.008945\n",
      "epoch 50         loss: 0.008299\n",
      "epoch 51         loss: 0.008066\n",
      "epoch 52         loss: 0.008427\n",
      "epoch 53         loss: 0.007138\n",
      "epoch 54         loss: 0.005154\n",
      "epoch 55         loss: 0.008429\n",
      "epoch 56         loss: 0.004857\n",
      "epoch 57         loss: 0.006541\n",
      "epoch 58         loss: 0.003921\n",
      "epoch 59         loss: 0.002960\n",
      "epoch 60         loss: 0.004793\n",
      "epoch 61         loss: 0.006508\n",
      "epoch 62         loss: 0.002907\n",
      "epoch 63         loss: 0.003638\n",
      "epoch 64         loss: 0.003553\n",
      "epoch 65         loss: 0.004550\n",
      "epoch 66         loss: 0.005040\n",
      "epoch 67         loss: 0.003695\n",
      "epoch 68         loss: 0.003217\n",
      "epoch 69         loss: 0.002761\n",
      "epoch 70         loss: 0.004340\n",
      "epoch 71         loss: 0.002527\n",
      "epoch 72         loss: 0.002915\n",
      "epoch 73         loss: 0.003361\n",
      "epoch 74         loss: 0.002136\n",
      "epoch 75         loss: 0.001735\n",
      "epoch 76         loss: 0.001236\n",
      "epoch 77         loss: 0.001598\n",
      "epoch 78         loss: 0.001847\n",
      "epoch 79         loss: 0.002718\n",
      "epoch 80         loss: 0.001260\n",
      "epoch 81         loss: 0.001980\n",
      "epoch 82         loss: 0.002600\n",
      "epoch 83         loss: 0.002275\n",
      "epoch 84         loss: 0.005997\n",
      "epoch 85         loss: 0.002029\n",
      "epoch 86         loss: 0.002080\n",
      "epoch 87         loss: 0.002413\n",
      "epoch 88         loss: 0.003350\n",
      "epoch 89         loss: 0.001718\n",
      "epoch 90         loss: 0.001921\n",
      "epoch 91         loss: 0.001193\n",
      "epoch 92         loss: 0.001776\n",
      "epoch 93         loss: 0.002656\n",
      "epoch 94         loss: 0.001969\n",
      "epoch 95         loss: 0.002723\n",
      "epoch 96         loss: 0.000621\n",
      "epoch 97         loss: 0.002641\n",
      "epoch 98         loss: 0.001491\n",
      "epoch 99         loss: 0.000687\n",
      "epoch 100        loss: 0.000663\n"
     ]
    }
   ],
   "source": [
    "use_gpu = True\n",
    "num_epoch = 100\n",
    "\n",
    "if use_gpu:\n",
    "    device = torch.device('cuda')\n",
    "else:\n",
    "    device = torch.device('cpu')\n",
    "    \n",
    "crnn.train()\n",
    "trainer = torch.optim.Adam(crnn.parameters(), lr=0.001)\n",
    "loss = nn.CTCLoss(zero_infinity=True)\n",
    "crnn = crnn.to(device)\n",
    "loss = loss.to(device)\n",
    "    \n",
    "for epoch in range(num_epoch):\n",
    "    for X, y in trainloader:\n",
    "        X = X.to(device)\n",
    "        trainer.zero_grad()\n",
    "        preds = crnn(X) \n",
    "        encoded_text, preds_length, actual_length = get_ctcloss_parameters(y)\n",
    "\n",
    "        encoded_text = encoded_text.to(device)\n",
    "        preds_length = preds_length.to(device)\n",
    "        actual_length = actual_length.to(device)\n",
    "        \n",
    "        l = loss(preds, encoded_text,preds_length, actual_length) / batch_size\n",
    "        l.backward()\n",
    "        trainer.step()\n",
    "    print('epoch', str(epoch + 1).ljust(10), 'loss:', format(l.item(), '.6f'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_final_pred(text):\n",
    "    \"\"\"Remove adjacent duplicate characters\n",
    "\n",
    "    Args:\n",
    "        text: Do argmax after crnn net ouput\n",
    "        \n",
    "    Returns:\n",
    "        final_text: Text removed adjacent duplicate characters\n",
    "    \"\"\"\n",
    "    text = list(text)\n",
    "    for i in range(len(text)):\n",
    "        for j in range(i + 1, len(text)):\n",
    "            if text[j] == ' ':\n",
    "                break\n",
    "            else:\n",
    "                if text[j] == text[i]:\n",
    "                    text[j] = ' '\n",
    "                else:\n",
    "                    continue\n",
    "    final_text = ''.join(text).replace(' ', '')\n",
    "    return final_text\n",
    "\n",
    "def predict(net, X, y):\n",
    "    \"\"\"Predict batch images, print predict result and ground truth.\n",
    "    \n",
    "    Args:\n",
    "        net: crnn net\n",
    "        X: batch images\n",
    "        y: batch actual texts\n",
    "    \"\"\"\n",
    "    preds = net(X)\n",
    "    _, preds = preds.max(2)\n",
    "    idx = 0\n",
    "    print('crnn net output'.ljust(51), '|', 'final predict'.ljust(20), '|', 'ground truth'.ljust(20))\n",
    "    print('=' * 99)\n",
    "    for pred in preds.permute(1, 0):\n",
    "        pred_text = ''.join([alphabet[i.item()] for i in pred])\n",
    "        print(pred_text, '|', get_final_pred(pred_text).ljust(20), '|', y[idx].ljust(20))\n",
    "        print('·' * 99)\n",
    "        idx += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crnn net output                                     | final predict        | ground truth        \n",
      "===================================================================================================\n",
      "Gaaaaaaaa9aaaaQaaaaaa9aaaaUaaaaaa | Ga9Q9U               | State               \n",
      "···································································································\n",
      "\n",
      "\n",
      "\n",
      "crnn net output                                     | final predict        | ground truth        \n",
      "===================================================================================================\n",
      "KaaaaaaaaaCaaaaaFaaaaazaaaraaaaaa | KaCFzr               | WORLD               \n",
      "···································································································\n",
      "gaaaaaakaaaaaabaaaaaaaiaaaaajaaaa | gakbij               | SOUTH               \n",
      "···································································································\n",
      "caaaaaaaaCaaaaaaagaaaaaaaagaaaaaa | caCgg                | JOE'S               \n",
      "···································································································\n",
      "qaaaaaaaaaaQaaaaaaaa7aaaaaUaaaaaa | qaQ7U                | Cafe                \n",
      "···································································································\n",
      "gaaaaaaaaaaCaaaaaaaaaaaaaaaaaaaaa | gaC                  | ELGIN               \n",
      "···································································································\n",
      "Kaaaaaaaaaaaaaawaaaaazaaaaaazaaaa | Kawzz                | WILL                \n",
      "···································································································\n",
      "IaaaaaaasaaaDaaaaaHaaavaaaaaGaaaa | IasDHvG              | DEPTHS              \n",
      "···································································································\n",
      "faaaaaaaaaaaaaaaaaaaaaaaaaaaaaaaa | fa                   | THE                 \n",
      "···································································································\n"
     ]
    }
   ],
   "source": [
    "test_set = MyDataset(data_dir='data/processed/ittk5/test')\n",
    "\n",
    "# predict single image with random index\n",
    "idx = random.randint(0, len(test_set) - 1)\n",
    "X, y = test_set[idx]\n",
    "X = X.unsqueeze(0) # add dim as batch\n",
    "y = [y]\n",
    "X = X.to(device)\n",
    "predict(crnn, X, y)\n",
    "print('\\n' * 2)\n",
    "# predict batch using dataloader\n",
    "testloader = DataLoader(test_set, batch_size=8, shuffle=True, drop_last=True)\n",
    "X, y = next(iter(testloader))\n",
    "X = X.to(device)\n",
    "predict(crnn, X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output: tensor([[[-6.1337e+00, -7.4148e+00, -7.6299e-01,  ..., -6.6640e+00,\n",
      "          -1.1636e+01, -1.1873e+01]],\n",
      "\n",
      "        [[-7.2000e-05, -1.4874e+01, -1.4837e+01,  ..., -1.6584e+01,\n",
      "          -1.7709e+01, -1.6164e+01]],\n",
      "\n",
      "        [[-2.3842e-06, -1.8139e+01, -1.7818e+01,  ..., -2.0427e+01,\n",
      "          -2.1621e+01, -1.9644e+01]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[-1.1921e-06, -1.8024e+01, -1.8447e+01,  ..., -2.1068e+01,\n",
      "          -2.1932e+01, -1.9875e+01]],\n",
      "\n",
      "        [[-2.5034e-06, -1.7182e+01, -1.7975e+01,  ..., -2.0359e+01,\n",
      "          -2.1145e+01, -1.9072e+01]],\n",
      "\n",
      "        [[-1.2755e-05, -1.5133e+01, -1.6552e+01,  ..., -1.8425e+01,\n",
      "          -1.8884e+01, -1.6827e+01]]], device='cuda:0')\n",
      "Model output shape: torch.Size([33, 1, 86])\n",
      "Max indices: tensor([2, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0], device='cuda:0')\n",
      "Prediction: c\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "\n",
    "# Define the transformation\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((32, 128)),  # Adjust height to 32 or as needed\n",
    "    transforms.ToTensor()\n",
    "])\n",
    "\n",
    "def load_image(image_path):\n",
    "    # Load the image\n",
    "    image = Image.open(image_path)\n",
    "    # Apply the transformations\n",
    "    image = transform(image)\n",
    "    # Add a batch dimension\n",
    "    image = image.unsqueeze(0)\n",
    "    return image\n",
    "\n",
    "def decode(output, alphabet):\n",
    "    # Assuming output is a tensor of shape (time_step, batch_size, num_class)\n",
    "    output = output.permute(1, 0, 2)  # Change to (batch_size, time_step, num_class)\n",
    "    output = output.squeeze(0)  # Remove batch dimension\n",
    "    _, max_indices = torch.max(output, dim=1)\n",
    "    \n",
    "    print(f'Max indices: {max_indices}')  # Debugging: print the max indices\n",
    "    \n",
    "    # Convert indices to characters and handle repeated characters\n",
    "    predicted_text = []\n",
    "    prev_idx = None\n",
    "    for idx in max_indices:\n",
    "        if idx != prev_idx and idx != 0:  # Skip repeated characters and blank character (assuming blank is index 0)\n",
    "            predicted_text.append(alphabet[idx])\n",
    "        prev_idx = idx\n",
    "    \n",
    "    return ''.join(predicted_text)\n",
    "\n",
    "def predict_single_image(model, image_path, alphabet):\n",
    "    # Load and preprocess the image\n",
    "    image = load_image(image_path)\n",
    "    # Move the image to the device\n",
    "    image = image.to(device)\n",
    "    # Set the model to evaluation mode\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Get the predictions\n",
    "        output = model(image)\n",
    "        print(f'Model output: {output}')  # Debugging: print the model output\n",
    "        print(f'Model output shape: {output.shape}')  # Debugging: print the output shape\n",
    "        # Decode the predictions\n",
    "        prediction = decode(output, alphabet)\n",
    "    return prediction\n",
    "\n",
    "# Example usage\n",
    "alphabet = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789 '  # Define your alphabet including a blank character\n",
    "image_path = 'data/raw/character_set1/Test_1.png'\n",
    "prediction = predict_single_image(crnn, image_path, alphabet)\n",
    "print('Prediction:', prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
