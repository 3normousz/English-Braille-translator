{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T08:42:30.262341Z",
     "start_time": "2024-11-16T08:42:30.257842Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, MaxPooling2D, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43859993d48bf350",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T08:42:33.140199Z",
     "start_time": "2024-11-16T08:42:32.071113Z"
    }
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "training_path = 'data/raw/character_set1/training_data/'\n",
    "testing_path = 'data/raw/character_set1/testing_data/'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43bc146db7c198a7",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T08:42:44.134995Z",
     "start_time": "2024-11-16T08:42:33.927582Z"
    }
   },
   "outputs": [],
   "source": [
    "dir_list = os.listdir(training_path)\n",
    "for i in dir_list:\n",
    "  dir = os.path.join(training_path, i)\n",
    "  file_list = os.listdir(dir)\n",
    "  for j in file_list:\n",
    "    files = os.path.join(dir, j)\n",
    "    img = cv2.imread(files)\n",
    "    img = cv2.resize(img, (64,64))\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    img = img/255\n",
    "    images.append(img)\n",
    "    labels.append(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e963705255a7dd",
   "metadata": {},
   "source": [
    "## Print out details of X and y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f793a21829d226d",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T08:42:49.100372Z",
     "start_time": "2024-11-16T08:42:44.996056Z"
    }
   },
   "outputs": [],
   "source": [
    "X = np.array(images)\n",
    "print(\"len(X): \",len(X))\n",
    "print(\"X.shape: \", X.shape)\n",
    "\n",
    "y = np.array(labels)\n",
    "print(\"len(y): \",len(y))\n",
    "print(\"y.shape: \", y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd15726c2948ad5",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T08:42:54.310010Z",
     "start_time": "2024-11-16T08:42:54.049587Z"
    }
   },
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "X_sh, y_sh = shuffle(X, y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc09ffaf114d8e14",
   "metadata": {},
   "source": [
    "# Create Keras model\n",
    "create a neural network model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6550f3e6e93b5b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T08:42:56.627907Z",
     "start_time": "2024-11-16T08:42:56.621382Z"
    }
   },
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a831ac19fad1390",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T08:42:58.358101Z",
     "start_time": "2024-11-16T08:42:58.268180Z"
    }
   },
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=16, kernel_size=(3,3), activation='relu', input_shape=(64,64,3)))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(filters=32, kernel_size=(3,3),  activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(filters=64, kernel_size=(3,3),  activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(filters=128, kernel_size=(3,3), activation='relu'))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(units=128, activation='relu'))\n",
    "model.add(Dense(units=64, activation='relu'))\n",
    "model.add(Dense(units=36, activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94e867168d5fe06",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T08:43:00.378903Z",
     "start_time": "2024-11-16T08:43:00.371263Z"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ebe91b9921e0b6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T08:44:16.966129Z",
     "start_time": "2024-11-16T08:43:02.380803Z"
    }
   },
   "outputs": [],
   "source": [
    "history = model.fit(X_sh, y_sh ,validation_split=0.2, batch_size=25, epochs=10)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c314eb075e9928a6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T08:44:21.369194Z",
     "start_time": "2024-11-16T08:44:20.688340Z"
    }
   },
   "outputs": [],
   "source": [
    "test_images = []\n",
    "test_labels = []\n",
    "\n",
    "dir_list = os.listdir(testing_path)\n",
    "for i in dir_list:\n",
    "  dir = os.path.join(testing_path, i)\n",
    "  file_list = os.listdir(dir)\n",
    "  for j in file_list:\n",
    "    files = os.path.join(dir, j)\n",
    "    img = cv2.imread(files)\n",
    "    img = cv2.resize(img, (64,64))\n",
    "    img = np.array(img, dtype=np.float32)\n",
    "    img = img/255\n",
    "    test_images.append(img)\n",
    "    test_labels.append(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41e7cd2e62b3b13b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T08:44:22.970310Z",
     "start_time": "2024-11-16T08:44:22.949147Z"
    }
   },
   "outputs": [],
   "source": [
    "X_test = np.array(test_images)\n",
    "y_test = np.array(test_labels)\n",
    "le = LabelEncoder()\n",
    "y_test = le.fit_transform(y_test) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbb0c61f777ab153",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-16T08:44:24.766814Z",
     "start_time": "2024-11-16T08:44:24.530101Z"
    }
   },
   "outputs": [],
   "source": [
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(test_loss,test_accuracy )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d35364e",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb07506",
   "metadata": {},
   "source": [
    "### Visualize Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60e0f1a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_regions(image, regions):\n",
    "    debug_image = image.copy()\n",
    "    for (x, y, w, h) in regions:\n",
    "        cv2.rectangle(debug_image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Detected Text Regions\", debug_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "914107cc",
   "metadata": {},
   "source": [
    "### Visualize Processed Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34884b7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def display_processed_image(window_name, processed_image):\n",
    "    cv2.imshow(window_name, processed_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c915dce9",
   "metadata": {},
   "source": [
    "## Image Processing Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330375c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def image_processing_operations_visualization(image, operation):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    if operation == 'threshold':\n",
    "        # Apply thresholding to get a binary image\n",
    "        _, processed_image = cv2.threshold(gray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "        window_name = \"Thresholded Image\"\n",
    "\n",
    "    elif operation == 'erosion':\n",
    "        # Apply thresholding and then erosion\n",
    "        _, thresh = cv2.threshold(gray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "        kernel = np.ones((2, 1), np.uint8)\n",
    "        processed_image = cv2.erode(thresh, kernel, iterations=1)\n",
    "        window_name = \"Eroded Image\"\n",
    "\n",
    "    elif operation == 'dilation':\n",
    "        # Apply thresholding and then dilation\n",
    "        _, thresh = cv2.threshold(gray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "        kernel = np.ones((2, 2), np.uint8)\n",
    "        processed_image = cv2.dilate(thresh, kernel, iterations=1)\n",
    "        window_name = \"Dilated Image\"\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Operation must be 'threshold', 'erosion', or 'dilation'\")\n",
    "\n",
    "    # Display the processed image\n",
    "    display_processed_image(window_name, processed_image)\n",
    "    \n",
    "    return _, processed_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff9ff14b",
   "metadata": {},
   "source": [
    "## Italic Characters Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869b5020",
   "metadata": {},
   "source": [
    "### Compute Skew Angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27c5f269",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_skew_angle(region):\n",
    "    # Use moments to calculate the skew angle of a text region\n",
    "    coords = np.column_stack(np.where(region > 0))\n",
    "    rect = cv2.minAreaRect(coords)\n",
    "    angle = rect[-1]\n",
    "    if angle < -45:\n",
    "        angle = -(90 + angle)\n",
    "    else:\n",
    "        angle = -angle\n",
    "    return angle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88b0f10",
   "metadata": {},
   "source": [
    "### Deskew Angle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c05a5969",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deskew_region(region, angle):\n",
    "    # Rotate the region by the computed angle\n",
    "    (h, w) = region.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(region, rotation_matrix, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_CONSTANT, borderValue=255)\n",
    "    return rotated"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad712c7e",
   "metadata": {},
   "source": [
    "## Detect Text Regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f76b77fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_text_regions(image):\n",
    "    # Apply image processing operation (e.g., thresholding, erosion, or dilation)\n",
    "    _, processed_image = image_processing_operations_visualization(image, 'threshold')\n",
    "    \n",
    "    # Find contours (regions of characters)\n",
    "    contours, _ = cv2.findContours(processed_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    regions = []\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        # region = processed_image[y:y+h, x:x+w]\n",
    "        \n",
    "        # Compute the skew angle of the detected region\n",
    "        # angle = compute_skew_angle(region)\n",
    "        \n",
    "        # Deskew the region if the skew angle is significant\n",
    "        # if abs(angle) > 5:  # Adjust the threshold based on your needs\n",
    "        #     region = deskew_region(region, angle)\n",
    "        \n",
    "        # Update bounding box after deskewing (if any change occurred)\n",
    "        # if abs(angle) > 5:\n",
    "        #     x, y, w, h = cv2.boundingRect(region)'''\n",
    "        \n",
    "        regions.append((x-1, y-16, w+1, h+16))\n",
    "    \n",
    "    # Visualize the image regions\n",
    "    visualize_regions(image, regions)\n",
    "    \n",
    "    return regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71c378b3",
   "metadata": {},
   "source": [
    "## Sorting Bounding Boxes by Rows and Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8198074",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sort_bounding_boxes(regions):\n",
    "    # Sort by `y` first (top-to-bottom) with a threshold to group by rows\n",
    "    row_threshold = 20  # Adjust based on character spacing\n",
    "    regions = sorted(regions, key=lambda box: box[1])\n",
    "\n",
    "    # Group bounding boxes into rows\n",
    "    rows = []\n",
    "    current_row = [regions[0]]\n",
    "    \n",
    "    for i in range(1, len(regions)):\n",
    "        if abs(regions[i][1] - current_row[-1][1]) < row_threshold:\n",
    "            current_row.append(regions[i])\n",
    "        else:\n",
    "            rows.append(current_row)\n",
    "            current_row = [regions[i]]\n",
    "    rows.append(current_row)\n",
    "    \n",
    "    # Sort each row left-to-right\n",
    "    sorted_regions = []\n",
    "    for row in rows:\n",
    "        sorted_row = sorted(row, key=lambda box: box[0])\n",
    "        sorted_regions.extend(sorted_row)\n",
    "    \n",
    "    return sorted_regions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d217243",
   "metadata": {},
   "source": [
    "## Recognize Characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "012cec5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recognize_characters(image, model, label_encoder):\n",
    "    regions = detect_text_regions(image)\n",
    "    \n",
    "    sorted_regions = sort_bounding_boxes(regions)\n",
    "    # print(sorted_regions)\n",
    "\n",
    "    characters = []\n",
    "    for (x, y, w, h) in sorted_regions:\n",
    "        char_image = image[y:y+h, x:x+w]\n",
    "        \n",
    "        # Add padding\n",
    "        # pad = 10\n",
    "        # char_image = cv2.copyMakeBorder(char_image, pad, pad, pad, pad, cv2.BORDER_CONSTANT, value=[255, 255, 255])\n",
    "        \n",
    "        char_image_resized = cv2.resize(char_image, (64, 64))\n",
    "        char_image_normalized = char_image_resized / 255.0\n",
    "\n",
    "        cv2.imshow('image after resized',char_image_normalized)\n",
    "        cv2.waitKey(0)  # Wait indefinitely for a key press\n",
    "        cv2.destroyAllWindows() \n",
    "        \n",
    "        prediction = model.predict(np.expand_dims(char_image_normalized, axis=0))\n",
    "        predicted_class = np.argmax(prediction, axis=1)\n",
    "        predicted_char = label_encoder.inverse_transform(predicted_class)[0]\n",
    "        \n",
    "        #To be decided later #If the current character is 'i' or 'j' we will\n",
    "        #pop the last element out (which is supposed to be the dot above small i and j)\n",
    "\n",
    "        # ----- UNCOMMENT LATER -----\n",
    "        # if predicted_char == 'i' or predicted_char == 'j'\n",
    "        #     characters.pop()\n",
    "        \n",
    "        characters.append(predicted_char)\n",
    "\n",
    "    return characters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4aa2d4fe",
   "metadata": {},
   "source": [
    "# Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e58f9fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "imggg = 'data/raw/character_set1/Test_2.jpg'\n",
    "image = cv2.imread(imggg) \n",
    "y_pred = recognize_characters(image, model, le)\n",
    "print(y_pred)\n",
    "print(len(y_pred))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor_flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
