{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Conv2D, Flatten, MaxPooling2D, Dense"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "43859993d48bf350",
   "metadata": {},
   "source": [
    "le = LabelEncoder()\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "training_paths = ['data/raw/character_set1/training_data/',\n",
    "                 'data/raw/character_set3/', ]\n",
    "#testing_path = 'data/raw/character_set1/testing_data/'\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "43bc146db7c198a7",
   "metadata": {},
   "source": [
    "for training_path in training_paths:\n",
    "    dir_list = os.listdir(training_path)\n",
    "    for i in dir_list:\n",
    "      dir = os.path.join(training_path, i)\n",
    "      file_list = os.listdir(dir)\n",
    "      for j in file_list:\n",
    "        files = os.path.join(dir, j)\n",
    "        img = cv2.imread(files)\n",
    "        img = cv2.resize(img, (64,64))\n",
    "        img = np.array(img, dtype=np.float32)\n",
    "        img = img/255\n",
    "        images.append(img)\n",
    "        labels.append(i)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "30e963705255a7dd",
   "metadata": {},
   "source": [
    "## Print out details of X and y"
   ]
  },
  {
   "cell_type": "code",
   "id": "7f793a21829d226d",
   "metadata": {},
   "source": [
    "X = np.array(images)\n",
    "print(\"len(X): \",len(X))\n",
    "print(\"X.shape: \", X.shape)\n",
    "\n",
    "y = np.array(labels)\n",
    "print(\"len(y): \",len(y))\n",
    "print(\"y.shape: \", y.shape)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "5bd15726c2948ad5",
   "metadata": {},
   "source": [
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "X_sh, y_sh = shuffle(X, y, random_state=42)"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Split the data: 80% training, 20% testing\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_sh, y_sh, test_size=0.2, random_state=42, stratify=y_sh\n",
    ")\n",
    "\n",
    "print(\"Training samples:\", len(X_train))\n",
    "print(\"Testing samples:\", len(X_test))"
   ],
   "id": "2836a9999f9df7d5",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "cc09ffaf114d8e14",
   "metadata": {},
   "source": [
    "# Create Keras model\n",
    "create a neural network model"
   ]
  },
  {
   "cell_type": "code",
   "id": "8a831ac19fad1390",
   "metadata": {},
   "source": [
    "model = Sequential([\n",
    "    Conv2D(filters=16, kernel_size=(3,3), activation='relu', input_shape=(64,64,3)),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(filters=32, kernel_size=(3,3), activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(filters=64, kernel_size=(3,3), activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Conv2D(filters=128, kernel_size=(3,3), activation='relu'),\n",
    "    MaxPooling2D(),\n",
    "    Flatten(),\n",
    "    Dense(units=128, activation='relu'),\n",
    "    Dense(units=64, activation='relu'),\n",
    "    Dense(units=len(le.classes_), activation='softmax')  # Dynamic output units\n",
    "])\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f94e867168d5fe06",
   "metadata": {},
   "source": [
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics = ['accuracy'])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "f2ebe91b9921e0b6",
   "metadata": {},
   "source": [
    "history = model.fit(X_sh, y_sh ,validation_split=0.2, batch_size=25, epochs=10)\n",
    "\n",
    "plt.plot(history.history['loss'])\n",
    "plt.plot(history.history['val_loss'])\n",
    "plt.legend(['loss', 'val_loss'])\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "c314eb075e9928a6",
   "metadata": {},
   "source": [
    "# Evaluate the model on the testing set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Evaluate the model on the testing set\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print(f\"Test Loss: {test_loss:.4f}\")\n",
    "print(f\"Test Accuracy: {test_accuracy:.4f}\")\n",
    "\n",
    "# Separate Evaluation for Uppercase and Lowercase Characters\n",
    "\n",
    "# Determine the number of uppercase letters\n",
    "# This assumes that the first 'num_uppercase' classes are uppercase letters\n",
    "# Adjust 'num_uppercase' based on your actual label encoding\n",
    "num_uppercase = 26  # Typically, A-Z\n",
    "\n",
    "# Find indices for uppercase and lowercase characters\n",
    "uppercase_indices = np.where(y_test < num_uppercase)\n",
    "lowercase_indices = np.where(y_test >= num_uppercase)\n",
    "\n",
    "# Evaluate the model on uppercase characters\n",
    "test_loss_upper, test_acc_upper = model.evaluate(X_test[uppercase_indices], y_test[uppercase_indices], verbose=0)\n",
    "print(f\"Uppercase Test Loss: {test_loss_upper:.4f}\")\n",
    "print(f\"Uppercase Test Accuracy: {test_acc_upper:.4f}\")\n",
    "\n",
    "# Evaluate the model on lowercase characters\n",
    "test_loss_lower, test_acc_lower = model.evaluate(X_test[lowercase_indices], y_test[lowercase_indices], verbose=0)\n",
    "print(f\"Lowercase Test Loss: {test_loss_lower:.4f}\")\n",
    "print(f\"Lowercase Test Accuracy: {test_acc_lower:.4f}\")\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "41e7cd2e62b3b13b",
   "metadata": {},
   "source": [
    "# Plot training & validation loss values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss Over Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot training & validation accuracy values\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy Over Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "8d35364e",
   "metadata": {},
   "source": [
    "## Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edb07506",
   "metadata": {},
   "source": [
    "### Visualize Regions"
   ]
  },
  {
   "cell_type": "code",
   "id": "60e0f1a6",
   "metadata": {},
   "source": [
    "def visualize_regions(image, regions):\n",
    "    debug_image = image.copy()\n",
    "    for (x, y, w, h) in regions:\n",
    "        cv2.rectangle(debug_image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "    cv2.imshow(\"Detected Text Regions\", debug_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "914107cc",
   "metadata": {},
   "source": [
    "### Visualize Processed Image"
   ]
  },
  {
   "cell_type": "code",
   "id": "34884b7f",
   "metadata": {},
   "source": [
    "def display_processed_image(window_name, processed_image):\n",
    "    cv2.imshow(window_name, processed_image)\n",
    "    cv2.waitKey(0)\n",
    "    cv2.destroyAllWindows()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "c915dce9",
   "metadata": {},
   "source": [
    "## Image Processing Operations"
   ]
  },
  {
   "cell_type": "code",
   "id": "330375c8",
   "metadata": {},
   "source": [
    "def image_processing_operations_visualization(image, operation):\n",
    "    # Convert image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "    if operation == 'threshold':\n",
    "        # Apply thresholding to get a binary image\n",
    "        _, processed_image = cv2.threshold(gray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "        window_name = \"Thresholded Image\"\n",
    "\n",
    "    elif operation == 'erosion':\n",
    "        # Apply thresholding and then erosion\n",
    "        _, thresh = cv2.threshold(gray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "        kernel = np.ones((2, 1), np.uint8)\n",
    "        processed_image = cv2.erode(thresh, kernel, iterations=1)\n",
    "        window_name = \"Eroded Image\"\n",
    "\n",
    "    elif operation == 'dilation':\n",
    "        # Apply thresholding and then dilation\n",
    "        _, thresh = cv2.threshold(gray, 100, 255, cv2.THRESH_BINARY_INV)\n",
    "        kernel = np.ones((1, 1), np.uint8)\n",
    "        processed_image = cv2.dilate(thresh, kernel, iterations=1)\n",
    "        window_name = \"Dilated Image\"\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"Operation must be 'threshold', 'erosion', or 'dilation'\")\n",
    "\n",
    "    # Display the processed image\n",
    "    display_processed_image(window_name, processed_image)\n",
    "    \n",
    "    return _, processed_image"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ff9ff14b",
   "metadata": {},
   "source": [
    "## Italic Characters Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "869b5020",
   "metadata": {},
   "source": [
    "### Compute Skew Angle"
   ]
  },
  {
   "cell_type": "code",
   "id": "27c5f269",
   "metadata": {},
   "source": [
    "def compute_skew_angle(region):\n",
    "    # Use moments to calculate the skew angle of a text region\n",
    "    coords = np.column_stack(np.where(region > 0))\n",
    "    rect = cv2.minAreaRect(coords)\n",
    "    angle = rect[-1]\n",
    "    if angle < -45:\n",
    "        angle = -(90 + angle)\n",
    "    else:\n",
    "        angle = -angle\n",
    "    return angle"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "f88b0f10",
   "metadata": {},
   "source": [
    "### Deskew Angle"
   ]
  },
  {
   "cell_type": "code",
   "id": "c05a5969",
   "metadata": {},
   "source": [
    "def deskew_region(region, angle):\n",
    "    # Rotate the region by the computed angle\n",
    "    (h, w) = region.shape[:2]\n",
    "    center = (w // 2, h // 2)\n",
    "    rotation_matrix = cv2.getRotationMatrix2D(center, angle, 1.0)\n",
    "    rotated = cv2.warpAffine(region, rotation_matrix, (w, h), flags=cv2.INTER_CUBIC, borderMode=cv2.BORDER_CONSTANT, borderValue=255)\n",
    "    return rotated"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "ad712c7e",
   "metadata": {},
   "source": [
    "## Detect Text Regions"
   ]
  },
  {
   "cell_type": "code",
   "id": "f76b77fc",
   "metadata": {},
   "source": [
    "def detect_text_regions(image):\n",
    "    # Apply image processing operation (e.g., thresholding, erosion, or dilation)\n",
    "    _, processed_image = image_processing_operations_visualization(image, 'dilation')\n",
    "    \n",
    "    # Find contours (regions of characters)\n",
    "    contours, _ = cv2.findContours(processed_image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "    regions = []\n",
    "    for contour in contours:\n",
    "        x, y, w, h = cv2.boundingRect(contour)\n",
    "        \n",
    "        # region = processed_image[y:y+h, x:x+w]\n",
    "        \n",
    "        # Compute the skew angle of the detected region\n",
    "        # angle = compute_skew_angle(region)\n",
    "        \n",
    "        # Deskew the region if the skew angle is significant\n",
    "        # if abs(angle) > 5:  # Adjust the threshold based on your needs\n",
    "        #     region = deskew_region(region, angle)\n",
    "        \n",
    "        # Update bounding box after deskewing (if any change occurred)\n",
    "        # if abs(angle) > 5:\n",
    "        #     x, y, w, h = cv2.boundingRect(region)'''\n",
    "        \n",
    "        # old\n",
    "        # regions.append((x-1, y, w+1, h))\n",
    "        \n",
    "        # new\n",
    "        if 0 < w < 100 and 0 < h < 100:  # Size filter\n",
    "            regions.append((x, y, w, h))\n",
    "    \n",
    "    # Visualize the image regions\n",
    "    visualize_regions(image, regions)\n",
    "    \n",
    "    return regions"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "71c378b3",
   "metadata": {},
   "source": [
    "## Sorting Bounding Boxes by Rows and Columns"
   ]
  },
  {
   "cell_type": "code",
   "id": "d8198074",
   "metadata": {},
   "source": [
    "def sort_bounding_boxes(regions):\n",
    "    # Sort by `y` first (top-to-bottom) with a threshold to group by rows\n",
    "    row_threshold = 20  # Adjust based on character spacing\n",
    "    regions = sorted(regions, key=lambda box: box[1])\n",
    "\n",
    "    # Group bounding boxes into rows\n",
    "    rows = []\n",
    "    current_row = [regions[0]]\n",
    "    \n",
    "    for i in range(1, len(regions)):\n",
    "        if abs(regions[i][1] - current_row[-1][1]) < row_threshold:\n",
    "            current_row.append(regions[i])\n",
    "        else:\n",
    "            rows.append(current_row)\n",
    "            current_row = [regions[i]]\n",
    "    rows.append(current_row)\n",
    "    \n",
    "    # Sort each row left-to-right\n",
    "    sorted_regions = []\n",
    "    for row in rows:\n",
    "        sorted_row = sorted(row, key=lambda box: box[0])\n",
    "        sorted_regions.extend(sorted_row)\n",
    "    \n",
    "    return sorted_regions\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "9a7f0c99",
   "metadata": {},
   "source": [
    "## Resize Image Operation"
   ]
  },
  {
   "cell_type": "code",
   "id": "64543a19",
   "metadata": {},
   "source": [
    "def resize_image(image, target_size=(64, 64), mode='normal'):\n",
    "    if mode == 'normal':\n",
    "        # Normal resize (ignores aspect ratio)\n",
    "        resized_image = cv2.resize(image, target_size)\n",
    "    elif mode == 'aspect_ratio':\n",
    "        # Resize while maintaining aspect ratio\n",
    "        (iH, iW) = image.shape[:2]\n",
    "        if iW > iH:\n",
    "            resized_image = cv2.resize(image, (target_size[0], int(target_size[0] * iH / iW)))\n",
    "        else:\n",
    "            resized_image = cv2.resize(image, (int(target_size[1] * iW / iH), target_size[1]))\n",
    "        \n",
    "        # After resizing, padding the image to make it exactly target_size\n",
    "        (iH, iW) = resized_image.shape\n",
    "        dX = int(max(0, target_size[0] - iW) / 2.0)\n",
    "        dY = int(max(0, target_size[1] - iH) / 2.0)\n",
    "        resized_image = cv2.copyMakeBorder(resized_image, dY, dY, dX, dX, cv2.BORDER_CONSTANT, value=[255, 255, 255])\n",
    "    else:\n",
    "        raise ValueError(\"Mode must be either 'normal' or 'aspect_ratio'\")\n",
    "    \n",
    "    return resized_image"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1d217243",
   "metadata": {},
   "source": [
    "## Recognize Characters"
   ]
  },
  {
   "cell_type": "code",
   "id": "012cec5b",
   "metadata": {},
   "source": [
    "def recognize_characters(image, model, label_encoder):\n",
    "    regions = detect_text_regions(image)\n",
    "    \n",
    "    sorted_regions = sort_bounding_boxes(regions)\n",
    "    # print(sorted_regions)\n",
    "\n",
    "    characters = []\n",
    "    for (x, y, w, h) in sorted_regions:\n",
    "        char_image = image[y:y+h, x:x+w]\n",
    "        \n",
    "        # Add padding\n",
    "        pad = 1\n",
    "        char_image = cv2.copyMakeBorder(char_image, pad, pad, pad, pad, cv2.BORDER_CONSTANT, value=[255, 255, 255])\n",
    "        \n",
    "        # Check if the char_image is empty\n",
    "        if char_image.size == 0:\n",
    "            print(f\"Skipping empty character region at ({x}, {y}, {w}, {h})\")\n",
    "            continue  # Skip this iteration if the character image is empty\n",
    "        \n",
    "        # Print the shape of the character image for debugging\n",
    "        print(f\"Character image shape: {char_image.shape}\")\n",
    "        \n",
    "        # Resize the character image\n",
    "        char_image_resized = resize_image(char_image, target_size=(64, 64), mode='normal')\n",
    "        char_image_normalized = char_image_resized / 255.0\n",
    "\n",
    "        cv2.imshow('image after resized',char_image_normalized)\n",
    "        cv2.waitKey(0)  # Wait indefinitely for a key press\n",
    "        cv2.destroyAllWindows() \n",
    "        \n",
    "        prediction = model.predict(np.expand_dims(char_image_normalized, axis=0))\n",
    "        predicted_class = np.argmax(prediction, axis=1)\n",
    "\n",
    "        if predicted_class[0] not in range(len(label_encoder.classes_)):\n",
    "            print(f\"Unrecognized label: {predicted_class[0]}\")\n",
    "            predicted_char = \"?\"  # Placeholder for unrecognized labels\n",
    "        else:\n",
    "            predicted_char = label_encoder.inverse_transform(predicted_class)[0]\n",
    "        \n",
    "        #To be decided later #If the current character is 'i' or 'j' we will\n",
    "        #pop the last element out (which is supposed to be the dot above small i and j)\n",
    "\n",
    "        # ----- UNCOMMENT LATER -----\n",
    "        # if predicted_char == 'i' or predicted_char == 'j'\n",
    "        #     characters.pop()\n",
    "        \n",
    "        characters.append(predicted_char)\n",
    "\n",
    "    return characters"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "4aa2d4fe",
   "metadata": {},
   "source": [
    "# Driver Code"
   ]
  },
  {
   "cell_type": "code",
   "id": "9e58f9fc",
   "metadata": {},
   "source": [
    "imggg = 'data/raw/character_set1/Test_1.png'\n",
    "image = cv2.imread(imggg) \n",
    "y_pred = recognize_characters(image, model, le)\n",
    "print(y_pred)\n",
    "print(len(y_pred))"
   ],
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensor_flow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
