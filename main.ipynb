{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns  #Unused import\n",
    "import os\n",
    "from symspellpy import SymSpell, Verbosity\n",
    "import pkg_resources\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Dense, Dropout, Conv2D, MaxPool2D, \n",
    "                                     BatchNormalization, Flatten, GlobalAveragePooling2D, Input,\n",
    "                                     LSTM, Bidirectional, TimeDistributed, Reshape, Embedding)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import EfficientNetB7, MobileNetV2, VGG19, DenseNet121"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directory_to_df(path : str):\n",
    "    \"\"\"\n",
    "    This function to retrieve all images from targeted folder in a file, the\n",
    "    folder must be divided hirarchally in which each class contains its images individually.\n",
    "    ________________________________________________________________________________________________\n",
    "    Arguments-\n",
    "    \n",
    "    path: String -> the main folder directory that contains train/test folders\n",
    "    \n",
    "    ________________________________________________________________________________________________\n",
    "    Return-\n",
    "    \n",
    "    DataFrame: contains the images path and label corresponding to every image\n",
    "    \"\"\"\n",
    "    df = []\n",
    "    chars = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789!,#&()-$%;@[]^_`}{~+'\n",
    "    for cls in os.listdir(path):\n",
    "        cls_path = os.path.join(path,cls)\n",
    "        cls_name = cls.split('_')[0]\n",
    "        if not cls_name in chars:\n",
    "            if cls_name == 'qmark':\n",
    "                cls_name = '?'\n",
    "            elif cls_name == 'dot':\n",
    "                cls_name = '.'\n",
    "            elif cls_name == 'colon':\n",
    "                cls_name = ':'\n",
    "            else:\n",
    "                continue\n",
    "        for img_path in os.listdir(cls_path):\n",
    "            direct = os.path.join(cls_path,img_path)\n",
    "            df.append([direct,cls_name])\n",
    "    \n",
    "    df = pd.DataFrame(df, columns=['image','label'])\n",
    "    print(\"The number of samples found:\",len(df))\n",
    "    return df.copy()\n",
    "\n",
    "def read_image(path):\n",
    "    \"\"\"\n",
    "    Read an image from specified directory\n",
    "    _____________________________________________________________\n",
    "    Arguments:\n",
    "    \n",
    "    path: String -> a directory of the image\n",
    "    _____________________________________________________________\n",
    "    Return:\n",
    "    \n",
    "    image: numpy.array of the image\n",
    "    \"\"\"\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def show_image(img, label=None) -> None:\n",
    "    \"\"\"\n",
    "    This function to display any image\n",
    "    _________________________________________________________\n",
    "    Arguements:\n",
    "    \n",
    "    img: numpy.array of N-D\n",
    "    \n",
    "    label: String -> the title/label added with the image, Default= None\n",
    "    _________________________________________________________\n",
    "    Return:\n",
    "    \n",
    "    plt.imshow()\n",
    "    \"\"\"\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis(False)\n",
    "    plt.title(label)\n",
    "    plt.show()\n",
    "    \n",
    "def clbck(model_name):\n",
    "    # The function is defined to make the callbacks for training the models\n",
    "    ERLY = EarlyStopping(patience=10, min_delta=0.01, start_from_epoch=10, verbose=1)\n",
    "    RD = ReduceLROnPlateau(patience=5, min_delta=0.01, factor=0.5)\n",
    "    CHK = ModelCheckpoint(f'models/{model_name}_model.keras',verbose=1, save_best_only=True)\n",
    "    return [ERLY,RD,CHK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-defined hyperparameters\n",
    "IMG_SHAPE = (32,32)\n",
    "IMG_SIZE = (32,32,3)\n",
    "BATCH_SIZE = 32\n",
    "opt = Adam(learning_rate=0.0001, epsilon=1e-6)\n",
    "loss = 'categorical_crossentropy'\n",
    "EPOCHS = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Reading & preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset in dataframe \n",
    "main_path = 'data/raw/character_set3/'\n",
    "df = directory_to_df(main_path)                   # convert the dataset into df of two columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1) Splitting the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting for training & testing (70,30 respectively)\n",
    "X, y = df['image'], df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y , test_size=0.30, random_state=41)\n",
    "training_df = pd.concat((X_train,y_train), axis=1)\n",
    "testing_df = pd.concat((X_test,y_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting for training & validation (75,25 respectively) -> the training set size = 52.5%\n",
    "X, y = training_df['image'], training_df['label']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X,y , test_size=0.25, random_state=41)\n",
    "training_df = pd.concat((X_train,y_train), axis=1)\n",
    "validation_df = pd.concat((X_valid,y_valid), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2) Creating generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating generators\n",
    "gen = ImageDataGenerator(\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    zoom_range=0.2,\n",
    "    brightness_range=[0.5, 1.5],\n",
    "    fill_mode='nearest',\n",
    "    dtype=np.int32\n",
    ")\n",
    "gen2 = ImageDataGenerator(dtype=np.int32, fill_mode='nearest')\n",
    "train_gen = gen.flow_from_dataframe(training_df, x_col='image',y_col='label', batch_size=BATCH_SIZE, \n",
    "                                   target_size=IMG_SHAPE)\n",
    "valid_gen = gen2.flow_from_dataframe(validation_df, x_col='image', y_col='label', batch_size=BATCH_SIZE, \n",
    "                                        target_size=IMG_SHAPE, shuffle=False)\n",
    "test_gen = gen2.flow_from_dataframe(testing_df, x_col='image', y_col='label', batch_size=BATCH_SIZE, \n",
    "                                       target_size=IMG_SHAPE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a mapping of the classes and the inverse for later processings\n",
    "mapping = train_gen.class_indices\n",
    "mapping_inverse = dict(map(lambda x: tuple(reversed(x)), mapping.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a sample from the dataset\n",
    "BATCH_NUM = 10\n",
    "IMG_NUM = 2      # from 0 to 31\n",
    "show_image(train_gen[BATCH_NUM][0][IMG_NUM],mapping_inverse[train_gen[BATCH_NUM][1][IMG_NUM].argmax()])\n",
    "print('The shape of the image:',train_gen[BATCH_NUM][0][IMG_NUM].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading another sample from the dataset\n",
    "BATCH_NUM = 65\n",
    "IMG_NUM = 30      # from 0 to 31\n",
    "show_image(train_gen[BATCH_NUM][0][IMG_NUM],mapping_inverse[train_gen[BATCH_NUM][1][IMG_NUM].argmax()])\n",
    "print('The shape of the image:',train_gen[BATCH_NUM][0][IMG_NUM].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "CNN_model = Sequential()\n",
    "CNN_model.add(Input(shape=IMG_SIZE, name='Input'))\n",
    "CNN_model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "CNN_model.add(BatchNormalization())\n",
    "CNN_model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "CNN_model.add(BatchNormalization())\n",
    "CNN_model.add(MaxPool2D((2,2)))\n",
    "CNN_model.add(Dropout(0.25))\n",
    "\n",
    "CNN_model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "CNN_model.add(BatchNormalization())\n",
    "CNN_model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "CNN_model.add(BatchNormalization())\n",
    "CNN_model.add(MaxPool2D((2,2)))\n",
    "CNN_model.add(Dropout(0.25))\n",
    "\n",
    "CNN_model.add(Flatten())\n",
    "CNN_model.add(Dense(256, activation='relu', kernel_regularizer=l2(0.001)))\n",
    "CNN_model.add(Dropout(0.5))\n",
    "CNN_model.add(Dense(len(mapping), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default parameters of adam will be used for the custom CNN\n",
    "CNN_model.compile(optimizer=Adam(), loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) Callback Name Setup\n",
    "This will allow you to call the model without having to re-train the model everytime. Simply change the `callback_name` to save a new model in the 'models' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the name of the model at this variable.\n",
    "callback_name = 'CustomCNN'\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    verbose=1, \n",
    "    min_lr=1e-6, \n",
    "    )\n",
    "\n",
    "callback = clbck(callback_name) + [reduce_lr] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3) Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different num. of epochs will be given for better convergence for the Custom CNN\n",
    "history = CNN_model.fit(train_gen, epochs=EPOCHS, validation_data=valid_gen, callbacks=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss value')\n",
    "plt.title(\"Custom CNN Training VS. Validation performance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a prediction out of the Custom CNN for the testing set for the evaluation\n",
    "prediction = CNN_model.predict(test_gen)\n",
    "pred = list(map(lambda x: mapping_inverse[np.argmax(x)], prediction))\n",
    "y_test = list(map(lambda x: mapping_inverse[x],test_gen.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t\\tThe Custom CNN Evaluation Performance')\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computer Vision - Low level techniques\n",
    "def load_model():\n",
    "    model_path = 'models/CustomCNN_model.keras'\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    return model\n",
    "\n",
    "def convert_2_gray(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    return gray_image\n",
    "\n",
    "def binarization(image):\n",
    "    img, thresh = cv2.threshold(image, 0,255, cv2.THRESH_OTSU|cv2.THRESH_BINARY_INV)\n",
    "    return img, thresh\n",
    "\n",
    "def dilate(image, words= False):\n",
    "    img = image.copy()\n",
    "    m = 3\n",
    "    n = m - 2                   # n less than m for Vertical structuring element to dilate chars\n",
    "    itrs = 4\n",
    "    if words:\n",
    "        m = 6\n",
    "        n = m\n",
    "        itrs = 3\n",
    "    rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (n, m))\n",
    "    dilation = cv2.dilate(img, rect_kernel, iterations = itrs)\n",
    "    return dilation\n",
    "\n",
    "def find_rect(image):\n",
    "    '''\n",
    "    find the text region in the image and return the coordinates of the rectangles in a sorted manner.\n",
    "    \n",
    "    Input: image -> numpy.array of the image\n",
    "    \n",
    "    Output: list of rectangles coordinates sorted from top-to-bottom, left-to-right.\n",
    "    '''\n",
    "    contours, hierarchy = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    rects = []\n",
    "    \n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        rects.append([x, y, w, h])\n",
    "    \n",
    "    if not rects:\n",
    "        return []\n",
    "    \n",
    "    # Calculate average height\n",
    "    avg_height = np.mean([r[3] for r in rects])\n",
    "    margin = avg_height / 2\n",
    "    \n",
    "    # Sort rectangles by y-coordinate\n",
    "    rects_sorted = sorted(rects, key=lambda r: r[1])\n",
    "    \n",
    "    lines = []\n",
    "    current_line = []\n",
    "    current_y = rects_sorted[0][1]\n",
    "    \n",
    "    for rect in rects_sorted:\n",
    "        x, y, w, h = rect\n",
    "        if abs(y - current_y) <= margin:\n",
    "            current_line.append(rect)\n",
    "        else:\n",
    "            # Sort the current line by x-coordinate\n",
    "            current_line = sorted(current_line, key=lambda r: r[0])\n",
    "            lines.append(current_line)\n",
    "            current_line = [rect]\n",
    "            current_y = y\n",
    "    # Add the last line\n",
    "    if current_line:\n",
    "        current_line = sorted(current_line, key=lambda r: r[0])\n",
    "        lines.append(current_line)\n",
    "    \n",
    "    # Flatten the list of lines\n",
    "    sorted_rects = [rect for line in lines for rect in line]\n",
    "    \n",
    "    return sorted_rects\n",
    "\n",
    "def extract(image):\n",
    "    model = load_model()\n",
    "    chars = []              # a list to store recognized characters\n",
    "    \n",
    "    image_cpy = image.copy()\n",
    "    _, bin_img = binarization(convert_2_gray(image_cpy))\n",
    "    full_dil_img = dilate(bin_img,words=True)\n",
    "    words = find_rect(full_dil_img)                       # Recognized words within the image \n",
    "    del _, bin_img, full_dil_img                          # for better memory usage\n",
    "    \n",
    "    for word in words:\n",
    "        x,y,w,h = word                                    # coordinates of the word\n",
    "        img = image_cpy[y:y+h, x:x+w]\n",
    "        \n",
    "        _, bin_img = binarization(convert_2_gray(img))\n",
    "        dil_img = dilate(bin_img)\n",
    "        char_parts = find_rect(dil_img)                     # Recognized chars withtin the word\n",
    "        cv2.rectangle(image, (x,y),(x+w,y+h), (0,255,0), 3) # draw a green rectangle around the word\n",
    "        \n",
    "        del _, bin_img, dil_img\n",
    "        \n",
    "        for char in char_parts:    \n",
    "            x,y,w,h = char\n",
    "            ch = img[y:y+h, x:x+w]\n",
    "            \n",
    "            empty_img = np.full((32,32,1),255, dtype=np.uint8) # a white image used for resize with filling\n",
    "            x,y = 3,3                                          # starting indecies\n",
    "            resized = cv2.resize(ch, (16,22), interpolation=cv2.INTER_CUBIC)\n",
    "            gray = convert_2_gray(resized)\n",
    "            empty_img[y:y+22, x:x+16,0] = gray.copy()          # integrate the recognized char into the white image\n",
    "            gray = cv2.cvtColor(empty_img, cv2.COLOR_GRAY2RGB)\n",
    "            gray = gray.astype(np.int32)\n",
    "            \n",
    "            predicted = mapping_inverse[np.argmax(model.predict(np.array([gray]), verbose=-1))]\n",
    "            chars.append(predicted)                            # append the character into the list\n",
    "            \n",
    "            del ch, resized, gray, empty_img\n",
    "        chars.append(' ')  # at the end of each iteration (end of word) append a space\n",
    "        \n",
    "    del model\n",
    "    show_image(image)\n",
    "    return ''.join(chars[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing 1 (Upper case + Lower case)\n",
    "img = read_image('data/test/Test_2.png')\n",
    "text = extract(img)\n",
    "print('-->',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing 2 (Lower case)\n",
    "img = read_image('data/raw/character_set1/Test_4.png')\n",
    "text = extract(img)\n",
    "print('-->',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell checking with SymSpell\n",
    "\n",
    "Symspell performs a spell checking on the OCR'ed text and correct text based on its dictionary database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "input_term = text\n",
    "result = sym_spell.word_segmentation(input_term)\n",
    "print('The corrected text:',result.corrected_string)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
