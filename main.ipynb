{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns  #Unused import\n",
    "import os\n",
    "#from symspellpy import SymSpell, Verbosity\n",
    "import pkg_resources\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import (Dense, Dropout, Conv2D, MaxPool2D, \n",
    "                                     BatchNormalization, Flatten, GlobalAveragePooling2D, Input,\n",
    "                                     LSTM, Bidirectional, TimeDistributed, Reshape, Embedding)\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, LearningRateScheduler, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import EfficientNetB7, MobileNetV2, VGG19, DenseNet121\n",
    "\n",
    "from tensorflow.keras.regularizers import l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def directory_to_df(path : str):\n",
    "    \"\"\"\n",
    "    This function to retrieve all images from targeted folder in a file, the\n",
    "    folder must be divided hirarchally in which each class contains its images individually.\n",
    "    ________________________________________________________________________________________________\n",
    "    Arguments-\n",
    "    \n",
    "    path: String -> the main folder directory that contains train/test folders\n",
    "    \n",
    "    ________________________________________________________________________________________________\n",
    "    Return-\n",
    "    \n",
    "    DataFrame: contains the images path and label corresponding to every image\n",
    "    \"\"\"\n",
    "    df = []\n",
    "    chars = 'abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ0123456789!,#&()-$%;@[]^_`}{~+'\n",
    "    for cls in os.listdir(path):\n",
    "        cls_path = os.path.join(path,cls)\n",
    "        cls_name = cls.split('_')[0]\n",
    "        if not cls_name in chars:\n",
    "            if cls_name == 'qmark':\n",
    "                cls_name = '?'\n",
    "            elif cls_name == 'dot':\n",
    "                cls_name = '.'\n",
    "            elif cls_name == 'colon':\n",
    "                cls_name = ':'\n",
    "            else:\n",
    "                continue\n",
    "        for img_path in os.listdir(cls_path):\n",
    "            direct = os.path.join(cls_path,img_path)\n",
    "            df.append([direct,cls_name])\n",
    "    \n",
    "    df = pd.DataFrame(df, columns=['image','label'])\n",
    "    print(\"The number of samples found:\",len(df))\n",
    "    return df.copy()\n",
    "\n",
    "def read_image(path):\n",
    "    \"\"\"\n",
    "    Read an image from specified directory\n",
    "    _____________________________________________________________\n",
    "    Arguments:\n",
    "    \n",
    "    path: String -> a directory of the image\n",
    "    _____________________________________________________________\n",
    "    Return:\n",
    "    \n",
    "    image: numpy.array of the image\n",
    "    \"\"\"\n",
    "    image = cv2.imread(path)\n",
    "    image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    return image\n",
    "\n",
    "def show_image(img, label=None) -> None:\n",
    "    \"\"\"\n",
    "    This function to display any image\n",
    "    _________________________________________________________\n",
    "    Arguements:\n",
    "    \n",
    "    img: numpy.array of N-D\n",
    "    \n",
    "    label: String -> the title/label added with the image, Default= None\n",
    "    _________________________________________________________\n",
    "    Return:\n",
    "    \n",
    "    plt.imshow()\n",
    "    \"\"\"\n",
    "    plt.imshow(img, cmap='gray')\n",
    "    plt.axis(False)\n",
    "    plt.title(label)\n",
    "    plt.show()\n",
    "    \n",
    "def clbck(model_name):\n",
    "    # The function is defined to make the callbacks for training the models\n",
    "    ERLY = EarlyStopping(patience=10, min_delta=0.01, start_from_epoch=10, verbose=1)\n",
    "    RD = ReduceLROnPlateau(\n",
    "        monitor='val_accuracy',\n",
    "        factor=0.5,\n",
    "        patience=3,\n",
    "        verbose=1, \n",
    "        min_lr=1e-6, \n",
    "    )\n",
    "    CHK = ModelCheckpoint(f'models/{model_name}_model.keras',verbose=1, save_best_only=True)\n",
    "    return [ERLY,RD,CHK]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-defined hyperparameters\n",
    "IMG_SHAPE = (64, 64)\n",
    "IMG_SIZE = (64, 64, 3)\n",
    "BATCH_SIZE = 32\n",
    "opt = Adam(learning_rate=0.0001, epsilon=1e-6)\n",
    "loss = 'categorical_crossentropy'\n",
    "EPOCHS = 50\n",
    "\n",
    "# Model selection\n",
    "# 1. EfficientNetB7\n",
    "# 2. CustomCNN\n",
    "# MODEL_SEL = 'EfficientNetB7'\n",
    "MODEL_SEL = 'NewCustomCNN'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3) Reading & preparing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading the dataset in dataframe \n",
    "main_path = 'data/raw/character_set3/'\n",
    "df = directory_to_df(main_path)                   # convert the dataset into df of two columns\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.1) Splitting the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting for training & testing (70,30 respectively)\n",
    "X, y = df['image'], df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X,y , test_size=0.30, random_state=41)\n",
    "training_df = pd.concat((X_train,y_train), axis=1)\n",
    "testing_df = pd.concat((X_test,y_test), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting for training & validation (75,25 respectively) -> the training set size = 52.5%\n",
    "X, y = training_df['image'], training_df['label']\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(X,y , test_size=0.25, random_state=41)\n",
    "training_df = pd.concat((X_train,y_train), axis=1)\n",
    "validation_df = pd.concat((X_valid,y_valid), axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3.2) Creating generators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating generators\n",
    "gen = ImageDataGenerator(\n",
    "    rotation_range=0,\n",
    "    shear_range=0,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0,\n",
    "    brightness_range=[0.5, 1.5],\n",
    "    fill_mode='nearest',\n",
    "    dtype=np.int32,\n",
    ")\n",
    "\n",
    "gen2 = ImageDataGenerator(dtype=np.int32, fill_mode='nearest')\n",
    "train_gen = gen.flow_from_dataframe(training_df, x_col='image',y_col='label', batch_size=BATCH_SIZE, \n",
    "                                   target_size=IMG_SHAPE)\n",
    "valid_gen = gen2.flow_from_dataframe(validation_df, x_col='image', y_col='label', batch_size=BATCH_SIZE, \n",
    "                                        target_size=IMG_SHAPE, shuffle=False)\n",
    "test_gen = gen2.flow_from_dataframe(testing_df, x_col='image', y_col='label', batch_size=BATCH_SIZE, \n",
    "                                       target_size=IMG_SHAPE, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a mapping of the classes and the inverse for later processings\n",
    "mapping = train_gen.class_indices\n",
    "mapping_inverse = dict(map(lambda x: tuple(reversed(x)), mapping.items()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading a sample from the dataset\n",
    "BATCH_NUM = 10\n",
    "IMG_NUM = 2      # from 0 to 31\n",
    "show_image(train_gen[BATCH_NUM][0][IMG_NUM],mapping_inverse[train_gen[BATCH_NUM][1][IMG_NUM].argmax()])\n",
    "print('The shape of the image:',train_gen[BATCH_NUM][0][IMG_NUM].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reading another sample from the dataset\n",
    "BATCH_NUM = 65\n",
    "IMG_NUM = 30      # from 0 to 31\n",
    "show_image(train_gen[BATCH_NUM][0][IMG_NUM],mapping_inverse[train_gen[BATCH_NUM][1][IMG_NUM].argmax()])\n",
    "print('The shape of the image:',train_gen[BATCH_NUM][0][IMG_NUM].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_NUM = 0\n",
    "IMG_NUM = 0      # from 0 to 31\n",
    "for i in range(32):\n",
    "    show_image(train_gen[BATCH_NUM][0][IMG_NUM+i],mapping_inverse[train_gen[BATCH_NUM][1][IMG_NUM+i].argmax()])\n",
    "    print('The shape of the image:',train_gen[BATCH_NUM][0][IMG_NUM+i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4) Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1) Custom Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.regularizers import l2\n",
    "\n",
    "CNN_model = Sequential()\n",
    "CNN_model.add(Input(shape=IMG_SIZE, name='Input'))\n",
    "CNN_model.add(Conv2D(32, (3,3), activation='relu', padding='same'))\n",
    "CNN_model.add(BatchNormalization())\n",
    "CNN_model.add(Conv2D(32, (3,3), activation='relu'))\n",
    "CNN_model.add(BatchNormalization())\n",
    "CNN_model.add(MaxPool2D((2,2)))\n",
    "CNN_model.add(Dropout(0.25))\n",
    "\n",
    "CNN_model.add(Conv2D(64, (3,3), activation='relu', padding='same'))\n",
    "CNN_model.add(BatchNormalization())\n",
    "CNN_model.add(Conv2D(64, (3,3), activation='relu'))\n",
    "CNN_model.add(BatchNormalization())\n",
    "CNN_model.add(MaxPool2D((2,2)))\n",
    "CNN_model.add(Dropout(0.25))\n",
    "\n",
    "CNN_model.add(Conv2D(128, (3,3), activation='relu', padding='same'))  # Added layer\n",
    "CNN_model.add(BatchNormalization())\n",
    "CNN_model.add(MaxPool2D((2,2)))\n",
    "CNN_model.add(Dropout(0.25))\n",
    "\n",
    "CNN_model.add(Flatten())\n",
    "CNN_model.add(Dense(512, activation='relu', kernel_regularizer=l2(0.001)))  # Increased units\n",
    "CNN_model.add(Dropout(0.5))\n",
    "CNN_model.add(Dense(len(mapping), activation='softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CNN_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default parameters of adam will be used for the custom CNN\n",
    "CNN_model.compile(optimizer=Adam(), loss=loss, metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2) Callback Name Setup\n",
    "This will allow you to call the model without having to re-train the model everytime. Simply change the `callback_name` to save a new model in the 'models' folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Change the name of the model at this variable.\n",
    "callback_name = 'NewCustomCNN'\n",
    "\n",
    "reduce_lr = ReduceLROnPlateau(\n",
    "    monitor='val_accuracy',\n",
    "    factor=0.5,\n",
    "    patience=3,\n",
    "    verbose=1, \n",
    "    min_lr=1e-6, \n",
    "    )\n",
    "\n",
    "callback = clbck(callback_name) + [reduce_lr] "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3) Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# different num. of epochs will be given for better convergence for the Custom CNN\n",
    "history = CNN_model.fit(\n",
    "    train_gen,\n",
    "    epochs=EPOCHS,\n",
    "    validation_data=valid_gen,\n",
    "    callbacks=clbck(MODEL_SEL),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(history.history['loss'], label='Training loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation loss')\n",
    "plt.legend()\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss value')\n",
    "plt.title(\"Custom CNN Training VS. Validation performance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Making a prediction out of the Custom CNN for the testing set for the evaluation\n",
    "prediction = CNN_model.predict(test_gen)\n",
    "pred = list(map(lambda x: mapping_inverse[np.argmax(x)], prediction))\n",
    "y_test = list(map(lambda x: mapping_inverse[x],test_gen.classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('\\t\\tThe Custom CNN Evaluation Performance')\n",
    "print(classification_report(y_test, pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6) Post-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Computer Vision - Low level techniques\n",
    "def load_model():\n",
    "    model_path = f'models/{MODEL_SEL}_model.keras'\n",
    "    model = tf.keras.models.load_model(model_path)\n",
    "    return model\n",
    "\n",
    "def convert_2_gray(image):\n",
    "    gray_image = cv2.cvtColor(image, cv2.COLOR_RGB2GRAY)\n",
    "    return gray_image\n",
    "\n",
    "def binarization(image):\n",
    "    img, thresh = cv2.threshold(image, 0,255, cv2.THRESH_OTSU|cv2.THRESH_BINARY_INV)\n",
    "    return img, thresh\n",
    "\n",
    "def dilate(image, words= False):\n",
    "    img = image.copy()\n",
    "    m = 3\n",
    "    n = m - 2                   # n less than m for Vertical structuring element to dilate chars\n",
    "    itrs = 4\n",
    "    if words:\n",
    "        m = 6\n",
    "        n = m\n",
    "        itrs = 3\n",
    "    rect_kernel = cv2.getStructuringElement(cv2.MORPH_RECT, (n, m))\n",
    "    dilation = cv2.dilate(img, rect_kernel, iterations = itrs)\n",
    "    return dilation\n",
    "\n",
    "def find_rect(image):\n",
    "    '''\n",
    "    find the text region in the image and return the coordinates of the rectangles in a sorted manner.\n",
    "    \n",
    "    Input: image -> numpy.array of the image\n",
    "    \n",
    "    Output: list of rectangles coordinates sorted from top-to-bottom, left-to-right.\n",
    "    '''\n",
    "    contours, hierarchy = cv2.findContours(image, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_NONE)\n",
    "    rects = []\n",
    "    \n",
    "    for cnt in contours:\n",
    "        x, y, w, h = cv2.boundingRect(cnt)\n",
    "        rects.append([x, y, w, h])\n",
    "    \n",
    "    if not rects:\n",
    "        return []\n",
    "    \n",
    "    # Calculate average height\n",
    "    avg_height = np.mean([r[3] for r in rects])\n",
    "    margin = avg_height / 2\n",
    "    \n",
    "    # Sort rectangles by y-coordinate\n",
    "    rects_sorted = sorted(rects, key=lambda r: r[1])\n",
    "    \n",
    "    lines = []\n",
    "    current_line = []\n",
    "    current_y = rects_sorted[0][1]\n",
    "    \n",
    "    for rect in rects_sorted:\n",
    "        x, y, w, h = rect\n",
    "        if abs(y - current_y) <= margin:\n",
    "            current_line.append(rect)\n",
    "        else:\n",
    "            # Sort the current line by x-coordinate\n",
    "            current_line = sorted(current_line, key=lambda r: r[0])\n",
    "            lines.append(current_line)\n",
    "            current_line = [rect]\n",
    "            current_y = y\n",
    "    # Add the last line\n",
    "    if current_line:\n",
    "        current_line = sorted(current_line, key=lambda r: r[0])\n",
    "        lines.append(current_line)\n",
    "    \n",
    "    # Flatten the list of lines\n",
    "    sorted_rects = [rect for line in lines for rect in line]\n",
    "    \n",
    "    return sorted_rects\n",
    "\n",
    "def extract(image):\n",
    "    model = load_model()\n",
    "    chars = []              # a list to store recognized characters\n",
    "    \n",
    "    image_cpy = image.copy()\n",
    "    _, bin_img = binarization(convert_2_gray(image_cpy))\n",
    "    full_dil_img = dilate(bin_img,words=True)\n",
    "    words = find_rect(full_dil_img)                       # Recognized words within the image \n",
    "    del _, bin_img, full_dil_img                          # for better memory usage\n",
    "    \n",
    "    for word in words:\n",
    "        x,y,w,h = word                                    # coordinates of the word\n",
    "        img = image_cpy[y:y+h, x:x+w]\n",
    "        \n",
    "        _, bin_img = binarization(convert_2_gray(img))\n",
    "        dil_img = dilate(bin_img)\n",
    "        char_parts = find_rect(dil_img)                     # Recognized chars withtin the word\n",
    "        cv2.rectangle(image, (x,y),(x+w,y+h), (0,255,0), 3) # draw a green rectangle around the word\n",
    "        \n",
    "        del _, bin_img, dil_img\n",
    "        \n",
    "        for char in char_parts:    \n",
    "            x,y,w,h = char\n",
    "            ch = img[y:y+h, x:x+w]\n",
    "            \n",
    "            empty_img = np.full((64,64,1),255, dtype=np.uint8) # a white image used for resize with filling\n",
    "            x,y = 3,3                                          # starting indecies\n",
    "            resized = cv2.resize(ch, (32,44), interpolation=cv2.INTER_CUBIC)\n",
    "            gray = convert_2_gray(resized)\n",
    "            empty_img[y:y+44, x:x+32,0] = gray.copy()          # integrate the recognized char into the white image\n",
    "            gray = cv2.cvtColor(empty_img, cv2.COLOR_GRAY2RGB)\n",
    "            gray = gray.astype(np.int32)\n",
    "            \n",
    "            #show_image(gray)\n",
    "            \n",
    "            predicted = mapping_inverse[np.argmax(model.predict(np.array([gray]), verbose=-1))]\n",
    "            chars.append(predicted)                            # append the character into the list\n",
    "            \n",
    "            del ch, resized, gray, empty_img\n",
    "        chars.append(' ')  # at the end of each iteration (end of word) append a space\n",
    "        \n",
    "    del model\n",
    "    show_image(image)\n",
    "    return ''.join(chars[:-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing 1 (Upper case + Lower case)\n",
    "img = read_image('data/test/Test_7.png')\n",
    "text = extract(img)\n",
    "print('-->',text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing 2 (Lower case)\n",
    "img = read_image('data/test/Test_3.png')\n",
    "text = extract(img)\n",
    "print('-->',text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spell checking with SymSpell\n",
    "\n",
    "Symspell performs a spell checking on the OCR'ed text and correct text based on its dictionary database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sym_spell = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n",
    "dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n",
    "sym_spell.load_dictionary(dictionary_path, term_index=0, count_index=1)\n",
    "\n",
    "input_term = text\n",
    "result = sym_spell.word_segmentation(input_term)\n",
    "print('The corrected text:',result.corrected_string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CER score\n",
    "\n",
    "Please run using the `compute_CER.py` file instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import compute_CER from CER.py located in helperFunctions/\n",
    "from computeCER import compute_CER\n",
    "\n",
    "predicted_text = \"LOrem ipSum dOLOr Sit ameto EX quiSquam quiSquam id eXpLiCabO iLLum hiC amet eVenieto HiC dOLOreS fugiat ut neSCiunt neSCiunt VeL pOSSimuS quiSquam eOS quam tempOribuS et repudiandae Veniam nOnWniam nemOo Qui Veniam dOLOrSed quia enimaut dOLOremque OffiCia qui aSpernatur impedit a enim tOtam At OptiO atqueo Id COnSequatur repudiandae hiCVOLuptate eLigendi et Cupiditate natuS eaOptiOdOLOremSed eXerCitatiOnemVeLito In eLigendi LabOrum et WLuptaS pariatur et quaSi rerum Sit quidem rerumo ESt magni animi eum inCidunt neque aut Sint VeritatiS eum mOdi WLuptaSl HiC diCta pariatur aut internOS LabOrum Sed OmniS repeLLat aut LabOrum nemO qui nObiStOtam nOn impedit ratiOne eSt QuiS tOtamo In quia VOLuptaS ea neque eLigendi et eXCepturi aSperiOreSo\"\n",
    "ground_truth = \"Lorem ipsum dolor sit amet. Ex quisquam quisquam id explicabo illum hic amet eveniet. Hic dolores fugiat ut nesciunt nesciunt vel possimus quisquam eos quam temporibus et repudiandae veniam non veniam nemo. Qui veniam dolor sed quia enim aut doloremque officia qui aspernatur impedit a enim totam At optio atque. Id consequatur repudiandae hic voluptate eligendi et cupiditate natus ea optio dolorem sed exercitationem velit. In eligendi laborum et voluptas pariatur et quasi rerum sit quidem rerum. Est magni animi eum incidunt neque aut sint veritatis eum modi voluptas! Hic dicta pariatur aut internos laborum sed omnis repellat aut laborum nemo qui nobis totam non impedit ratione est Quis totam. In quia voluptas ea neque eligendi et excepturi asperiores.\"\n",
    "\n",
    "print(compute_CER(predicted_text, ground_truth))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
